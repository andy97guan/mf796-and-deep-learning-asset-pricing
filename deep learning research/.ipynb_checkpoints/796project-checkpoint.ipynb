{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "# SGD part\n",
    "SGD_LSTM_OUTPUT = 4\n",
    "SGD_W_LAYER_1 = 64\n",
    "SGD_W_LAYER_2 = 64\n",
    "SGD_W_OUTPUT = 1\n",
    "\n",
    "# Conditional part\n",
    "CON_LSTM_OUTPUT = 4\n",
    "CON_G_OUTPUT = 4\n",
    "\n",
    "# learning parameter\n",
    "learning_rate = 0.001\n",
    "dropout_size = 0.95\n",
    "\n",
    "timesteps = 100 # days\n",
    "data_dim = 100 # features\n",
    "\n",
    "# output\n",
    "output = np.zeros((1000, 50*8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD 神经网络\n",
    "\n",
    "其中包括 LSTM，输出为4个状态，再加入一个64-64-1的DNN，然后构建$M_{t+1}R_{t+1}$，结合给定的$g$函数，计算均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑1000天，50只股票，100个微观因子，10个宏观因子\n",
    "# SGD networks\n",
    "macro_in1 = Input(shape=(10,))\n",
    "macro_in1_1 = Lambda(lambda x:K.reshape(x, shape=(-1,1, 10)))(macro_in1)\n",
    "\n",
    "micro_in1 = Input(shape=(50,100))\n",
    "\n",
    "return_next_date1 = Input(shape=(50,))\n",
    "\n",
    "sgd_lstm_out = LSTM(4)(macro_in1_1) # (1000, 4)\n",
    "sgd_lstm_out = Lambda(lambda x:K.repeat(x, 50))(sgd_lstm_out)\n",
    "\n",
    "sgd_weights_input = concatenate([micro_in1, sgd_lstm_out]) # (1000, 50, 104)  \n",
    "\n",
    "sgd_weights_output = Dense(64, activation='relu')(sgd_weights_input)\n",
    "sgd_weights_output = Dense(64, activation='relu')(sgd_weights_input)\n",
    "sgd_weights_output = Dense(1)(sgd_weights_input)\n",
    "sgd_weights_output = Lambda(lambda x:K.reshape(x, shape=(-1, 50)))(sgd_weights_output)\n",
    "\n",
    "\n",
    "def construction(x):\n",
    "    tmp = 1 - x[0] * x[1]\n",
    "    tmp = K.sum(tmp, axis=1)\n",
    "    tmp = K.reshape(tmp, shape=(-1,1)) # (1000, 1)\n",
    "    tmp = K.repeat(tmp, 50) # (1000, 50, 1)\n",
    "    tmp = K.reshape(tmp, shape=(-1,50)) # (1000, 50)\n",
    "    tmp = tmp * x[1] # (1000, 50)\n",
    "    tmp = K.reshape(tmp, shape=(-1, 50, 1))\n",
    "    return tmp # the M_{t+1}R_{t+1}\n",
    "\n",
    "sgd_construction_ouput = Lambda(construction)([sgd_weights_output,return_next_date1])\n",
    "\n",
    "\n",
    "## attain con_g_output \n",
    "con_g_output_loaded = Input(shape=(50, 8))\n",
    "\n",
    "loss_function_w = Lambda(lambda x:x[0]*x[1])([sgd_construction_ouput, con_g_output_loaded]) # (1000,50, 8)\n",
    "loss_function_w = Lambda(lambda x:K.reshape(x, shape=(-1, 400)))(loss_function_w)\n",
    "\n",
    "model_output_w = Model(inputs=[macro_in1, micro_in1], outputs=sgd_weights_output) # acquires weights given info\n",
    "\n",
    "model_output_sgd = Model(inputs=[macro_in1, micro_in1, return_next_date1], outputs=sgd_construction_ouput) # acquires MR for condition networks\n",
    "\n",
    "model_w = Model(inputs=[macro_in1, micro_in1, return_next_date1, con_g_output_loaded], outputs=loss_function_w)\n",
    "model_w.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 条件神经网络\n",
    "\n",
    "其中包括 LSTM，输出为4个状态，再加入一个64-8的DNN，然后构建$g$，结合给定的$M_{t+1}R_{t+1}$，计算均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional networks\n",
    "macro_in2 = Input(shape=(10,))\n",
    "macro_in2_1 = Lambda(lambda x:K.reshape(x, shape=(-1,1, 10)))(macro_in2)\n",
    "\n",
    "micro_in2 = Input(shape=(50,100))\n",
    "\n",
    "return_next_date2 = Input(shape=(50,))\n",
    "\n",
    "sgd_construction_ouput_loaded = Input(shape=(50,1))\n",
    "\n",
    "con_lstm_out = LSTM(4)(macro_in2_1) # (1000, 4)\n",
    "con_lstm_out = Lambda(lambda x:K.repeat(x, 50))(con_lstm_out)\n",
    "\n",
    "con_weights_input = concatenate([micro_in2, con_lstm_out]) # (1000, 50, 104)  \n",
    "\n",
    "con_g_output = Dense(64, activation='relu')(con_weights_input)\n",
    "con_g_output = Dense(8)(con_g_output) # (1000, 50, 8)\n",
    "\n",
    "loss_function_g = Lambda(lambda x:1/(0.001+x[0]*x[1]))([sgd_construction_ouput_loaded, con_g_output]) # (1000,50, 8)\n",
    "loss_function_g = Lambda(lambda x:K.reshape(x, shape=(-1, 400)))(loss_function_g)\n",
    "\n",
    "model_output_g = Model(inputs=[macro_in2, micro_in2], outputs=con_g_output) # acquires MR for condition networks\n",
    "\n",
    "model_g = Model(inputs=[macro_in2, micro_in2, return_next_date2, sgd_construction_ouput_loaded], outputs=loss_function_g)\n",
    "model_g.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 69us/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0000e+00\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 438us/step - loss: 999999.9375\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 999999.9375\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 88us/step - loss: 999999.9375\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 999999.9375\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 999999.9375\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 999999.93 - 0s 83us/step - loss: 999999.9375\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 86us/step - loss: 999999.9375\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 83us/step - loss: 999999.9375\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 85us/step - loss: 999999.9375\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 75us/step - loss: 999999.9375\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 74us/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 72us/step - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0000e+00\n",
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 999999.9375\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 87us/step - loss: 999999.9375\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 999999.9375\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 82us/step - loss: 999999.9375\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 89us/step - loss: 999999.9375\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 999999.9375\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 999999.9375\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 84us/step - loss: 999999.9375\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 79us/step - loss: 999999.9375\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 93us/step - loss: 999999.9375\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_train = np.zeros((1000, 400))\n",
    "macro_in = np.zeros((1000, 10))\n",
    "micro_in = np.zeros((1000, 50, 100))\n",
    "return_next_date = np.zeros((1000, 50))\n",
    "function_g = np.zeros((1000, 50, 8))\n",
    "for i in range(2):\n",
    "    # SDF nets\n",
    "    model_w.fit([macro_in, micro_in, return_next_date, function_g], y_train, epochs=10, batch_size=200)\n",
    "    function_sgd = model_output_sgd.predict([macro_in, micro_in, return_next_date])\n",
    "    print(function_sgd)\n",
    "    # conditional nets\n",
    "    model_g.fit([macro_in, micro_in, return_next_date, function_sgd], y_train, epochs=10, batch_size=200)\n",
    "    function_g = model_output_g.predict([macro_in, micro_in])\n",
    "    \n",
    "# get the optimal weights(SDF weights)\n",
    "print(model_output_w.predict([macro_in, micro_in])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
