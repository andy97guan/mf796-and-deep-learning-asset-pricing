{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SDF in Asset Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import relative package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. import micro, macro and return data from local\n",
    "data comes from Wind.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-be4bb12145b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "file = open('a','rb')\n",
    "dataset = pickle.load(file, encoding='utf-8')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_data, micro_data, return_data = dataset\n",
    "return_data = np.array(return_data)\n",
    "macro_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset split\n",
    "train_micro = micro_data[:2000, :, :]\n",
    "train_macro = macro_data[:2000, :, :]\n",
    "train_return = return_data[1:2001, :]\n",
    "\n",
    "validation_micro = micro_data[2001:2600, :, :]\n",
    "validation_macro = macro_data[2001:2600, :, :]\n",
    "validation_return = return_data[2002:2601, :]\n",
    "\n",
    "test_micro = micro_data[2601:-1, :, :]\n",
    "test_macro = macro_data[2601:-1, :, :]\n",
    "test_return = return_data[2602:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameter\n",
    "ndays = train_micro.shape[0]\n",
    "nstocks = train_micro.shape[1]\n",
    "nmacro = train_macro.shape[2]\n",
    "nmicro = train_micro.shape[2]\n",
    "LSTM_delay = 13\n",
    "n_g = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 SGD Network\n",
    "\n",
    "Including one LSTM to deal with Macro-variables with 4 hidden states，then the both the hidden states and the micro-variables go in to a 64-64-1 DNN. After that we are able to construct $M_{t+1}R_{t+1}$，and with the given $g$ function，we can calculate and minimize the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD networks\n",
    "\n",
    "# data loading\n",
    "macro_in1 = Input(shape=(LSTM_delay, nmacro))\n",
    "# macro_in1_1 = Lambda(lambda x:K.reshape(x, shape=(-1,1, nmacro)))(macro_in1)\n",
    "\n",
    "micro_in1 = Input(shape=(nstocks,nmicro))\n",
    "\n",
    "return_next_date1 = Input(shape=(nstocks,))\n",
    "\n",
    "\n",
    "# LSTM\n",
    "sgd_lstm_out = LSTM(4)(macro_in1) # (1000, 4)\n",
    "sgd_lstm_out = Lambda(lambda x:K.repeat(x, nstocks))(sgd_lstm_out)\n",
    "\n",
    "\n",
    "# SGD weights DNN\n",
    "sgd_weights_input = concatenate([micro_in1, sgd_lstm_out]) # (1000, 50, 104)  \n",
    "\n",
    "sgd_weights_output = Dense(64, activation='relu')(sgd_weights_input)\n",
    "sgd_weights_output = Dense(64, activation='relu')(sgd_weights_input)\n",
    "sgd_weights_output = Dense(1)(sgd_weights_input)\n",
    "sgd_weights_output = Lambda(lambda x:K.reshape(x, shape=(-1, nstocks)))(sgd_weights_output)\n",
    "\n",
    "\n",
    "# SGD construction\n",
    "def construction(x):\n",
    "    tmp = 1-x[0] * x[1]\n",
    "    tmp = K.sum(tmp, axis=1)\n",
    "    tmp = K.reshape(tmp, shape=(-1,1)) # (1000, 1)\n",
    "    tmp = K.repeat(tmp, nstocks) # (1000, 50, 1)\n",
    "    tmp = K.reshape(tmp, shape=(-1,nstocks)) # (1000, 50)\n",
    "    tmp = tmp * x[1] # (1000, 50)\n",
    "    tmp = K.reshape(tmp, shape=(-1, nstocks, 1))\n",
    "    return tmp # the M_{t+1}R_{t+1}\n",
    "\n",
    "sgd_construction_ouput = Lambda(construction)([sgd_weights_output,return_next_date1])\n",
    "\n",
    "\n",
    "# attain con_g_output from conditional network\n",
    "con_g_output_loaded = Input(shape=(nstocks, n_g))\n",
    "\n",
    "\n",
    "# combine those two and calculate loss\n",
    "loss_function_w = Lambda(lambda x:x[0]*x[1])([sgd_construction_ouput, con_g_output_loaded]) # (1000,50, 8)\n",
    "loss_function_w = Lambda(lambda x:K.reshape(x, shape=(-1, nstocks*n_g)))(loss_function_w)\n",
    "\n",
    "\n",
    "# with weights output for validation and sgd output for condition network training\n",
    "model_output_w = Model(inputs=[macro_in1, micro_in1], outputs=sgd_weights_output) # acquires weights given info\n",
    "model_output_sgd = Model(inputs=[macro_in1, micro_in1, return_next_date1], outputs=sgd_construction_ouput) # acquires MR for condition networks\n",
    "\n",
    "\n",
    "# SGD model compile\n",
    "model_w = Model(inputs=[macro_in1, micro_in1, return_next_date1, con_g_output_loaded], outputs=loss_function_w)\n",
    "model_w.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Conditional Neural Nets\n",
    "\n",
    "Including one LSTM for dealing with Macro-variables with 4 hidden states, then the hidden states and micro-variables goin to a 64-8 DNN，and finally with given $g$，we can calculate $M_{t+1}R_{t+1}$，and maximize the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional networks\n",
    "\n",
    "macro_in2 = Input(shape=(LSTM_delay, nmacro))\n",
    "\n",
    "micro_in2 = Input(shape=(nstocks,nmicro))\n",
    "\n",
    "sgd_construction_ouput_loaded = Input(shape=(nstocks,1))\n",
    "\n",
    "con_lstm_out = LSTM(4)(macro_in2) # (1000, 4)\n",
    "con_lstm_out = Lambda(lambda x:K.repeat(x, nstocks))(con_lstm_out)\n",
    "\n",
    "con_weights_input = concatenate([micro_in2, con_lstm_out]) # (1000, 50, 104)  \n",
    "\n",
    "con_g_output = Dense(64, activation='relu')(con_weights_input)\n",
    "con_g_output = Dense(n_g)(con_g_output) # (1000, 50, 8)\n",
    "\n",
    "loss_function_g = Lambda(lambda x:K.log(1/(1+K.abs(x[0]*x[1]))))([sgd_construction_ouput_loaded, con_g_output]) # (1000,50, 8)\n",
    "loss_function_g = Lambda(lambda x:K.reshape(x, shape=(-1, nstocks*n_g)))(loss_function_g)\n",
    "\n",
    "model_output_g = Model(inputs=[macro_in2, micro_in2], outputs=con_g_output) # acquires MR for condition networks\n",
    "\n",
    "model_g = Model(inputs=[macro_in2, micro_in2, sgd_construction_ouput_loaded], outputs=loss_function_g)\n",
    "model_g.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 data training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "y_train = np.zeros((ndays, n_g*nstocks))\n",
    "macro_in = train_macro\n",
    "micro_in = train_micro\n",
    "return_next_date = train_return\n",
    "function_g = np.ones((ndays, nstocks, n_g))\n",
    "sdf_loss = []\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 200\n",
    "n_iteration = 10\n",
    "\n",
    "for i in range(n_iteration):\n",
    "    t1 = time.time()\n",
    "\n",
    "    # SDF nets\n",
    "    history_w = model_w.fit([macro_in, micro_in, return_next_date, function_g], y_train, epochs=epoch, batch_size=batch_size, verbose=0)\n",
    "    function_sgd = model_output_sgd.predict([macro_in, micro_in, return_next_date])\n",
    "    sdf_loss = sdf_loss + history_w.history['loss'].copy() # save loss\n",
    "\n",
    "    # conditional nets\n",
    "    model_g.fit([macro_in, micro_in, function_sgd], y_train, epochs=epoch, batch_size=batch_size, verbose=0)\n",
    "    function_g = model_output_g.predict([macro_in, micro_in])\n",
    "    print('done with', i,',sdf loss is', sdf_loss[-1], ', using', round(time.time()-t1, 2), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights model\n",
    "\n",
    "# model_json = model_output_w.to_json()\n",
    "# with open(\"model_w.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "# with open(\"model_w.json\", \"r\") as json_file:\n",
    "#     mode_s = model_from_json(json_file.read())\n",
    "# mode_s.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the trend of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(sdf_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set condition\n",
    "print(model_output_w.predict([train_macro[:,:], train_micro[:, :, :]]).shape)\n",
    "train_weights = model_output_w.predict([train_macro[:,:], train_micro[:, :, :]])\n",
    "train_weights = 1/(1+np.exp(-train_weights))\n",
    "# train_yield = train_weights\n",
    "train_daily_return = (train_weights/train_weights.sum(axis=1).reshape(-1,1) * train_return).sum(axis=1)\n",
    "print(train_daily_return.mean(), train_daily_return.std(), train_daily_return.shape)\n",
    "\n",
    "benchmark = train_return.mean(axis=1)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.cumprod(1+train_daily_return), color='red',label='sdf')\n",
    "plt.plot(np.cumprod(1+benchmark), color='blue',label='benchmark')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(train_daily_return.mean()/train_daily_return.std()*np.sqrt(252), benchmark.mean()/benchmark.std()*np.sqrt(252))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set condition\n",
    "print(model_output_w.predict([validation_macro[:,:], validation_micro[:, :, :]]).shape)\n",
    "validation_weights = model_output_w.predict([validation_macro[:,:], validation_micro[:, :, :]])\n",
    "validation_weights = 1/(1+np.exp(-validation_weights))\n",
    "# train_yield = train_weights\n",
    "validation_daily_return = (validation_weights/validation_weights.sum(axis=1).reshape(-1,1) * validation_return).sum(axis=1)\n",
    "print(validation_daily_return.mean(), validation_daily_return.std(), validation_daily_return.shape)\n",
    "\n",
    "benchmark = validation_return.mean(axis=1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumprod(1+validation_daily_return), color='red',label='sdf')\n",
    "plt.plot(np.cumprod(1+benchmark), color='blue',label='benchmark')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(validation_daily_return.mean()/validation_daily_return.std()*np.sqrt(252), benchmark.mean()/benchmark.std()*np.sqrt(252))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set condition\n",
    "print(model_output_w.predict([test_macro[:,:], test_micro[:, :, :]]).shape)\n",
    "test_weights = model_output_w.predict([test_macro[:,:], test_micro[:, :, :]])\n",
    "test_weights = 1/(1+np.exp(-test_weights))\n",
    "# train_yield = train_weights\n",
    "test_daily_return = (test_weights/test_weights.sum(axis=1).reshape(-1,1) * test_return).sum(axis=1)\n",
    "print(test_daily_return.mean(), test_daily_return.std(), test_daily_return.shape)\n",
    "\n",
    "benchmark = test_return.mean(axis=1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.cumprod(1+test_daily_return), color='red',label='sdf')\n",
    "plt.plot(np.cumprod(1+benchmark), color='blue',label='benchmark')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(test_daily_return.mean()/test_daily_return.std()*np.sqrt(252), benchmark.mean()/benchmark.std()*np.sqrt(252))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
